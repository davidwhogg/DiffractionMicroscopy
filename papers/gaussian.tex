%% This file is part of the DiffractionMicroscopy project
%% Copyright 2015 David W. Hogg

% To-Do list
% ----------
% - merge in abstract from blindunslice.tex
% - merge in references from blindunslice.tex
% - perhaps re-phrase everything in terms of INVERSE variance?

% Style notes
% -----------

\documentclass[12pt]{article}
\usepackage{amssymb, mathrsfs, url}
\input{vc}

% useless formatting
\linespread{1.08333} % 10/13 spacing
\setlength{\parindent}{2\baselineskip}\addtolength{\parindent}{-1.25ex}
\setlength{\parskip}{0ex}

% math definitions
\newcommand{\normal}{\mathscr{N}}
\newcommand{\Poisson}{\mathscr{P}}
\newcommand{\sqnorm}[1]{|{#1}|^2}
\newcommand{\unitvec}[1]{\hat{#1}}
\newcommand{\ehat}{\unitvec{e}}
\newcommand{\xhat}{\unitvec{x}}
\newcommand{\yhat}{\unitvec{y}}
\newcommand{\zhat}{\unitvec{z}}
\newcommand{\transpose}{^{\mathsf{T}}}
\newcommand{\inverse}{^{-1}}
\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\dd}{\mathrm{d}}

\begin{document}\sloppy\sloppypar

\section*{\raggedright%
  Inferring the squared norm of a three-dimensional Fourier transform,
  from slices taken at unknown Euler angles: The Gaussian case}
\noindent
David W. Hogg (NYU) (SCDA) (MPIA)

\bigskip

\section{Problem statement}

In a three-dimensional (3-d) space there exists a 3-d multivariate
Gaussian-shaped density blob $f(x)$.
It's Fourier transform $F(k)$ in 3-d reciprocal space $k$ will
also be a Gaussian-shaped density blob.
\begin{eqnarray}
  F(k) &=& a\,\normal(k\given 0, V)
  \quad ,
\end{eqnarray}
where $a$ is some kind of amplitude and $\normal(k\given\mu, V)$ is a
Gaussian pdf for $k$ with 3-vector mean $\mu$ and $3\times 3$ variance
tensor $V$.
Without loss of generality we are going to presume a variance tensor
$V$ with principal axes aligned with the coordinate axes $\ehat_1$,
$\ehat_2$, and $\ehat_3$ directions:
\begin{eqnarray}
  V &=& \sum_{d=1}^3 \sigma^2_d \, \ehat_d\cdot\ehat_d\transpose
  \quad ,
\end{eqnarray}
where $V$ is a symmetric $3\times3$ tensor, the vectors are column
orthonormal unit vectors (so the vector products
$\ehat_d\cdot\ehat_d\transpose$ are outer products), the $\sigma^2_d$ are
scalar eigenvalues (principal variances).
Still without loss of generality we can assume that the corresponding
eigenvalues of the tensor are ordered
\begin{eqnarray}
  \sigma^2_1 \geq \sigma^2_2 \geq \sigma^2_3 > 0
  \quad .
\end{eqnarray}
Just as a reminder, the eigenvalues $\sigma^2_d$ have dimensions of
inverse length squared, since they are $k$-space variances.

We don't get to observe this Gaussian blob or its variance tensor
directly.
Instead, we get $N$ two-dimensional (2-d) images $y_n$, each of which
contains a noisy 2-d slice of the 3-d Gaussian blob, through the origin
but along an unknown slice plane.
In the small-angle limit of diffraction imaging, this slice is along a
plane perpendicular to the illumination direction $\zhat$.
In full generality, this slice is along a Ewald sphere that kisses
that perpendicular plane at the origin.
Let's start in the small-angle limit, to keep things simple.

In detail, when we take the small-angle limit, we can think of each
image $y_n$ as being an array of $M$ pixels at
2-vector coordinates $\xi_m$ in the slice plane.
Each image pixel $m$ contains a number of photons that can be thought
of as a Poisson draw with expectation set by the evaluation of the
squared norm $\sqnorm{F(k)}$ of the three-dimensional Gaussian
blob, restricted to the $\zhat$ slice, arbitrarily rotated (in 2-d).
(Actually, in detail, each pixel has expectation set by an
\emph{integral} of the squared norm of the blob, over the surface of
the pixel; we will ignore this integral for now, and just evaluate.)
None of the projection directions $\zhat_n$ or 2-d rotations are
known; only the noisy images are given.

The variance tensor of the 2-d Gaussian blob appearing in image $y_n$
is given by a slice and rotation of the 3-d inverse-variance tensor:
\begin{eqnarray}
  V_n &=& [P_n\transpose\cdot V^{-1}\cdot P_n]^{-1}
  \quad ,
\end{eqnarray}
where $P_n$ is a $3\times 2$ rectangular projection matrix, that
performs the slice and rotation.
Each rectangular projection matrix $P_n$ has three degrees of freedom or three
parameters (in some sense), which can be thought of as the Euler
angles, which are two angles on the sphere to define the direction of
$\zhat_n$ and one more angle to define the direction of the x-axis of
the image in the projection plane.

In detail, if one has the orthonormal unit 3-vectors
$\xhat_n,\yhat_n,\zhat_n$ that define the x-direction on the image
slice plane (but in the 3-d $k$-space), the y-direction on the image
slice plane, and the normal to the image slice plane, the $3\times 2$
projection matrix $P_n$ becomes
\begin{eqnarray}
  P_n &\equiv& [\xhat_n\,,\,\yhat_n]
  \quad ,
\end{eqnarray}
which (because $\xhat_n,\yhat_n$ are 3-vectors) is a $3\times2$ matrix.
This object $P_n$ might look like it has six parameters, but it is
constructed from two orthonormal vectors, so it really only has three
degrees of freedom, which correspond to the Euler angles.

DWH: SHOW EXAMPLE DATA HERE.

The question is:
Given these data, \emph{can we infer the 3-d variance tensor $V$?}
And if we can, how accurately, and what does it depend on?  We are
going to attempt the inference by maximizing a marginalized
likelihood, marginalizing out all $3\,N$ unknown projection and
rotation parameters.

\section{Marginalized likelihood}

Our single-pixel likelihood will be based on the Poisson process:
\begin{eqnarray}
  p(y_{nm}\given P_n,V) &=& \Poisson(y_{nm}\given\lambda_{nm})
  \\
  \lambda_{nm} &\equiv& \sqnorm{a\,\normal(\xi_m\given 0,V_n)}\,\Delta_m
  \quad,
\end{eqnarray}
where $y_{nm}$ is the $m$th pixel value in image $y_n$,
$\lambda_{nm}$ is the evaluation of the squared norm of the Fourier
transform at the pixel center (just evaluation, not integral),
$\Delta_m$ is the effective $k$-space area of the $m$th pixel,
and $\Poisson(k\given\lambda)$ is the Poisson probability for obtaining
count $k$ given expectation $\lambda$.
The likelihood is a function of the projections $P_n$ and the
three-space tensor $V$, through their combination $V_n$.

Amusingly, in the small-pixel limit (that is, in the limit in which
the image plane is dense with tiny pixels), the image $y_n$ just
becomes a photon list, containing $K_n$ photons $k$, each with
position $\xi_{nk}$.

If we drop (don't infer) normalizations, and presume that all pixels
have the same effective area $\Delta_m$ in $k$-space, then this leads
to a log-likelihood
\begin{eqnarray}
  \ln p(y_n\given P_n,V) &=& \sum_{m=1}^M \ln p(y_{nm}\given P_n,V)
  \\
  \ln p(y_{nm}\given P_n,V) &=& y_{nm}\,\ln \sqnorm{\normal(\xi_m\given 0,V_n)} + \mbox{const}
  \\
  \ln p(y_{nm}\given P_n,V) &=& y_{nm}\,\ln \normal(\xi_m\given 0,\frac{1}{2}\,V_n) + \mbox{const}
  \quad,
\end{eqnarray}
where we are ignoring constants, and where we have used that the square
of a Gaussian pdf is proportional to a Gaussian pdf with half the variance.
In the dense-image-of-small-pixels limit, this can be re-written as a
sum over photons, or...
\begin{eqnarray}
  \ln p(y_n\given P_n,V) &=& \sum_{k=1}^{K_n} \ln p(y_{nk}\given P_n,V)
  \\
  \ln p(y_{nk}\given P_n,V) &=& \ln \normal(\xi_{nk}\given 0,\frac{1}{2}\,V_n) + \mbox{const}
  \quad.
\end{eqnarray}

Now what we really want is to marginalize out the projections and
offsets.
We are going to think of the rectangular projection matrix $P_n$ as
being controlled by a set of Euler angles $\phi$ and we are going to
marginalize these out, using an isotropic prior.
\begin{eqnarray}
  \ln p(y_n\given V) &=& \ln \int p(y_n\given P_{\phi},V)\,p(\phi)\,\dd\phi
  \quad,
\end{eqnarray}
where now we subscript $P_{\phi}$ with the Euler angles and integrate
over them.
This marginalized likelihood has the horrifying form of a log of an
integral of an exponential of a simple log-likelihood.  That's bad.

We make things little better by replacing this integral with a
sampling approximation.
If we have $T$ samples $P_t$ of rectangular projection matrices, drawn
from an isotropic distribution of Euler angles $\phi$, the
marginalized likelihood can be approximated by
\begin{eqnarray}
  \ln p(y_n\given V) &=& \ln\left[\frac{1}{T}\,\sum_{t=1}^T p(y_n\given P_t,V)\right]
  \quad,
\end{eqnarray}
where now we have a log-sum-exp form (since it is $\ln p(y_n\given
P,V)$ that has the simple form).

\bigskip

Everything in this project, including this document itself, is
available at \giturl.  If you want this exact version, clone at git
hash \texttt{\githash~(\gitdate)}.

\end{document}
