%% This file is part of the DiffractionMicroscopy project
%% Copyright 2015 David W. Hogg

% To-Do list
% ----------

% Style notes
% -----------

\documentclass[12pt]{article}
\usepackage{amssymb, mathrsfs, url}
\input{vc}

% useless formatting
\linespread{1.08333} % 10/13 spacing
\setlength{\parindent}{2\baselineskip}\addtolength{\parindent}{-1.25ex}
\setlength{\parskip}{0ex}

% math definitions
\newcommand{\normal}{\mathscr{N}}
\newcommand{\Poisson}{\mathscr{P}}
\newcommand{\ft}[1]{\tilde{#1}}
\newcommand{\sqnorm}[1]{|{#1}|^2}
\newcommand{\unitvec}[1]{\hat{#1}}
\newcommand{\ehat}{\unitvec{e}}
\newcommand{\xhat}{\unitvec{x}}
\newcommand{\yhat}{\unitvec{y}}
\newcommand{\zhat}{\unitvec{z}}
\newcommand{\transpose}{^{\mathsf{T}}}
\newcommand{\inverse}{^{-1}}
\newcommand{\given}{\,|\,}
\newcommand{\like}{L}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\dd}{\mathrm{d}}

\begin{document}\sloppy\sloppypar

\section*{\raggedright%
  Inferring the squared norm of a three-dimensional Fourier transform,
  from slices taken at unknown Euler angles: The Gaussian case}
\noindent
David W. Hogg (NYU) (SCDA) (MPIA)

\bigskip

\section{Problem statement}

In a three-dimensional (3-d) space there exists a 3-d multivariate
Gaussian-shaped density blob $\rho(x)$.
It's Fourier transform $\ft{\rho}(k)$ in 3-d reciprocal space $k$ will
also be a Gaussian-shaped density blob.
\begin{eqnarray}
  \ft{\rho}(k) &=& a\,\normal(k\given 0, V)
  \quad ,
\end{eqnarray}
where $a$ is some kind of amplitude and $\normal(k\given\mu, V)$ is a
Gaussian pdf for $k$ with 3-vector mean $\mu$ and $3\times 3$ variance
tensor $V$.
Without loss of generality we are going to presume a variance tensor
$V$ with principal axes aligned with the coordinate axes $\ehat_1$,
$\ehat_2$, and $\ehat_3$ directions:
\begin{eqnarray}
  V &=& \sum_{d=1}^3 \sigma^2_d \, \ehat_d\cdot\ehat_d\transpose
  \quad ,
\end{eqnarray}
where $V$ is a symmetric $3\times3$ tensor, the vectors are column
orthonormal unit vectors (so the vector products
$\ehat_d\cdot\ehat_d\transpose$ are outer products), the $\sigma^2_d$ are
scalar eigenvalues (principal variances).
Still without loss of generality we can assume that the corresponding
eigenvalues of the tensor are ordered
\begin{eqnarray}
  \sigma^2_1 \geq \sigma^2_2 \geq \sigma^2_3 > 0
  \quad .
\end{eqnarray}
Just as a reminder, the eigenvalues $\sigma^2_d$ have dimensions of
inverse length squared, since they are $k$-space variances.

We don't get to observe this Gaussian blob or its variance tensor
directly.
Instead, we get $N$ two-dimensional (2-d) images $y_n$, each of which
contains a noisy 2-d slice of the 3-d Gaussian blob, along an unknown
slice plane.
In the small-angle limit of diffraction imaging, this slice is along a
plane perpendicular to the illumination direction $\hat{z}$.
In full generality, this slice is along a Ewald sphere that kisses
that perpendicular plane.
Let's start in the small-angle limit, to keep things simple.

In detail, when we take the small-angle limit, we can think of each
image $y_n$ as being an array of $M$ pixels at
2-vector coordinates $\xi_m$ in the slice plane.
Each image pixel $m$ contains a number of photons that can be thought
of as a Poisson draw with expectation set by the evaluation of the
squared norm $\sqnorm{\ft{\rho}(k)}$ of the three-dimensional Gaussian
blob, restricted to the $\hat{z}$ slice, arbitrarily rotated (in 2-d).
(Actually, in detail, each pixel has expectation set by an
\emph{integral} of the squared norm of the blob, over the surface of
the pixel; we will ignore this integral for now, and just evaluate.)
None of the projection directions $\zhat_n$ or 2-d rotations are
known; only the noisy images are given.

The variance tensor of the 2-d Gaussian blob appearing in image $y_n$
is given by a slice and rotation of the 3-d variance tensor:
\begin{eqnarray}
  V_n &=& P_n\transpose\cdot V\cdot P_n
  \quad ,
\end{eqnarray}
where $P_n$ is a $3\times 2$ rectangular projection matrix, that
performs the slice and rotation.
Each projection matrix $P_n$ have three degrees of freedom or three
parameters (in some sense), which can be thought of as the Euler
angles, which are two angles on the sphere to define the direction of
$\zhat_n$ and one more angle to define the direction of the x-axis of
the image in the projection plane.

DWH: INSERT MATH HERE.

DWH: SHOW EXAMPLE DATA HERE.

The question is:
Given these data, \emph{can we infer the 3-d variance tensor $V$?}
And if we can, how accurately, and what does it depend on?  We are
going to attempt the inference by maximizing a marginalized
likelihood, marginalizing out all $3\,N$ unknown projection and
rotation parameters.

\section{Marginalized likelihood}

Our single-pixel likelihood will be based on the Poisson process:
\begin{eqnarray}
  p(y_{nm}\given P_n,V) &=& \Poisson(y_{nm}\given\lambda_{nm})
  \\
  \lambda_{nm} &=& \sqnorm{a\,\normal(\xi_m\given 0,V_n)}\,\Delta_m
  \\
  \Poisson(k\given\lambda) &=& \frac{\lambda^k\,e^{-\lambda}}{k!}
  \quad,
\end{eqnarray}
where $y_{nm}$ is the $m$th pixel value in image $y_n$,
$\lambda_{nm}$ is the evaluation of the squared norm of the Fourier
transform at the pixel center (just evaluation, not integral),
$\Delta_m$ is the effective $k$-space area of the $m$th pixel,
and $\Poisson(k\given\lambda)$ is the Poisson probability.
The likelihood is a function of the projections $P_n$ and the
three-space tensor $V$, through their combination $V_n$.

DWH: If we drop (don't infer) normalizations, then this leads to a
log-likelihood...

DWH: Now what we really want is to marginalize out the projections and
offsets...

\end{document}

The marginalized log-likelihood (again, dropping a constant) looks like
\begin{eqnarray}
  -2\,\ln\like(a,V) &=& -2\,\sum_{n=1}^N\ln\int\exp(-\frac{1}{2}\,\sum_{m=1}^M \frac{[y_{nm} - Y_{nm}]^2}{\sigma^2})\,p(\phi_n)\,\dd\phi_n
  \quad,
\end{eqnarray}
where $p(\phi)$ is the prior over the 5 nuisance parameters for any
individual datum, and the likelihood is now a function only of
$\setof{a,V}$ because we have marginalized out all $5\,N$ nuisance
parameters.
Note that this horror is a log of an integral of an exponential of
something simple.
That's going to hurt (and going to suggest to our competitors to use
E-M, but we won't be so weak).
The prior $p(\phi)$ over nuisance parameters will be isotropic in
projection angles, isotropic in rotations in the 2-d space, and
uninformative---somehow---in offsets.

Imagine we had a large, dense, fair set of $K$ samples
$\phi_k\equiv\setof{P_k, \mu_k}$ from the prior $p(\phi)$ over
nuisance parameters.
Then we could approximate the marginalization integrals with sums, and
obtain (once again dropping a constant)
\begin{eqnarray}
  -2\,\ln\like(a,V) &\approx& -2\,\sum_{n=1}^N\ln\frac{1}{K}\,\sum_{k=1}^K\exp(-\frac{1}{2}\,\sum_{m=1}^M \frac{[y_{nm} - Y_{km}]^2}{\sigma^2})
  \\
  Y_{km} &\equiv& a\,\normal(\xi_m\given\mu_k,V_k)
  \\
  V_k &\equiv& P_k\transpose\cdot V\cdot P_k
  \quad,
\end{eqnarray}
where now the logsumexp has reared its head.

DWH: Now notice that we don't need the full set of samples to
approximate this integral well.

\section{Method and implementation}

DWH: To make fake data, we do the following...

DWH: How to re-sample from the prior, cleanly?  Or burn in?  Or whatever.

DWH: Do we have to re-sample from the full prior, or can we sample in
a neighborhood?  IF the latter, how do we figure out whether our
neighborhood was big enough?

DWH: How to initialize?

\section{Experiments}

\section{Discussion}


\bigskip

Everything in this project, including this document itself, is
available at \giturl.  If you want this exact version, clone at git
hash \texttt{\githash~(\gitdate)}.

\end{document}
